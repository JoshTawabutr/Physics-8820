{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Example: Parallel tempering for multimodal distributions\n",
    "\n",
    "Adapted from the TALENT course on Learning from Data: Bayesian Methods and Machine Learning, held in York, UK, June 10-28, 2019.\n",
    "The original notebook was by Christian Forssen.  Revisions are by Dick Furnstahl for Physics 8805/8820."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE: This version of the notebook uses the [ptemcee](https://github.com/willvousden/ptemcee) sampler that was forked from emcee when version 3 was released.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns;  sns.set(context='talk')\n",
    "\n",
    "import emcee\n",
    "import ptemcee\n",
    "import corner\n",
    "\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "from matplotlib import cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prior\n",
    "Use a Gaussian prior centered at (0,0) with variance 10. So this is supposed to encompass the entire region where we expect significant posterior probability. In particular, in the example, it will include both Gaussian peaks. $\\theta_0$ and $\\theta_1$ are assumed to be uncorrelated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mup = np.zeros(2)   # means\n",
    "sigp=np.sqrt(10.)   # standard deviation\n",
    "sigmap = np.diag([sigp**2, sigp**2]) # uncorrelated, so diagonal\n",
    "sigmapinv = np.linalg.inv(sigmap)\n",
    "\n",
    "# Normalization factor for the Gaussian\n",
    "normp = 1/np.sqrt(np.linalg.det(2*np.pi*sigmap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_prior(x):\n",
    "    \"\"\"\n",
    "    Define the log prior using linear algebra, normp and sigmapinv defined globally.\n",
    "    \"\"\" \n",
    "    dxp = x - mup\n",
    "    return -dxp @ sigmapinv @ dxp /2 + np.log(normp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_modes(sig=0.2, ratio=3.):\n",
    "    \"\"\"\n",
    "    Set up two Gaussians for the likelihood\n",
    "    \"\"\"\n",
    "    # Means of the two Gaussian modes\n",
    "    mu1 = np.ones(2)\n",
    "    mu2 = -np.ones(2)\n",
    "\n",
    "    # Width in each dimension (zero correlation)\n",
    "    _sig = sig\n",
    "    sigma1 = np.diag([_sig**2, _sig**2])\n",
    "    sigma2 = np.diag([_sig**2, _sig**2])\n",
    "    sigma1inv = np.linalg.inv(sigma1)\n",
    "    sigma2inv = np.linalg.inv(sigma2)\n",
    "\n",
    "    # amplitudes of the two modes and the corresponding norm factor\n",
    "    _amp1 = ratio / (1+ratio)\n",
    "    _amp2 = 1. / (1+ratio)\n",
    "    norm1 = _amp1 / np.sqrt(np.linalg.det(2*np.pi*sigma1))\n",
    "    norm2 = _amp2 / np.sqrt(np.linalg.det(2*np.pi*sigma2))\n",
    "    return (mu1,mu2,sigma1inv,sigma2inv,norm1,norm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_modes = setup_modes()  # use the defaults\n",
    "\n",
    "# Define the log likelihood function\n",
    "def log_likelihood(x, params=params_modes):\n",
    "    \"\"\"\n",
    "    Define the log likelihoo function using linear algebra.\n",
    "    \"\"\"\n",
    "    (mu1,mu2,sigma1inv,sigma2inv,norm1,norm2) = params # split out the parameters\n",
    "    dx1 = x - mu1\n",
    "    dx2 = x - mu2\n",
    "    return np.logaddexp(-dx1 @ sigma1inv @ dx1 / 2 + np.log(norm1), \\\n",
    "                        -dx2 @ sigma2inv @ dx2 / 2 + np.log(norm2) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posterior "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_posterior(x):\n",
    "    \"\"\"\n",
    "    Return the log of the posterior, calling the log prior and log likelihood \n",
    "    functions.\n",
    "    \"\"\"\n",
    "    return log_prior(x) + log_likelihood(x)\n",
    "\n",
    "@np.vectorize\n",
    "def posterior(y,x):\n",
    "    \"\"\"\n",
    "    Return the posterior by exponentiating the log prior and likelihood.\n",
    "    \"\"\"\n",
    "    xv=np.array([x,y])\n",
    "    return np.exp(log_likelihood(xv) + log_prior(xv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MH Sampling and convergence\n",
    "\n",
    "First do some basic sampling using the Metropolis-Hastings (MH) option in `emcee`. This is specified by `moves=emcee.moves.GaussianMove(cov)`. The starting positions are randomly distributed from 0 to 1 (not negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('emcee sampling (version: )', emcee.__version__)\n",
    "\n",
    "ndim = 2  # number of parameters in the model\n",
    "nwalkers = 50\n",
    "nwarmup = 1000\n",
    "nsteps = 1000\n",
    "\n",
    "# MH-Sampler setup\n",
    "stepsize = .05\n",
    "cov = stepsize * np.eye(ndim)\n",
    "p0 = np.random.rand(nwalkers, ndim)   # uniform between 0 and 1\n",
    "\n",
    "# initialize the sampler\n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, log_posterior, \n",
    "                               moves=emcee.moves.GaussianMove(cov))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample the posterior distribution\n",
    "\n",
    "# Warm-up\n",
    "if nwarmup > 0:\n",
    "    print(f'Performing {nwarmup} warmup iterations.')\n",
    "    pos, prob, state = sampler.run_mcmc(p0, nwarmup)\n",
    "    sampler.reset()\n",
    "else:\n",
    "    pos = p0\n",
    "    \n",
    "# Perform iterations, starting at the final position from the warmup.\n",
    "print(f'MH sampler performing {nsteps} samples.')\n",
    "%time sampler.run_mcmc(pos, nsteps)\n",
    "print(\"done\")\n",
    "\n",
    "print(f\"Mean acceptance fraction: {np.mean(sampler.acceptance_fraction):.3f}\")\n",
    "\n",
    "samples = sampler.chain.reshape((-1, ndim))\n",
    "samples_unflattened = sampler.chain\n",
    "lnposts = sampler.lnprobability.flatten()\n",
    "\n",
    "    \n",
    "# make a corner plot with the posterior distribution\n",
    "fig = corner.corner(samples, quantiles=[0.16, 0.5, 0.84], labels=[r\"$\\theta_0$\", r\"$\\theta_1$\"],\n",
    "                       show_titles=True, title_kwargs={\"fontsize\": 12})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** *According to this MCMC sampling, what does the posterior look like?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix, ax = plt.subplots(2,2,figsize=(12,5*ndim))\n",
    "for irow in range(ndim):\n",
    "    ax[irow,0].plot(np.arange(samples.shape[0]),samples[:,irow])\n",
    "    ax[irow,0].set_ylabel(r'$\\theta_{0}$'.format(irow))\n",
    "    ax[irow,1].hist(samples[:,irow],orientation='horizontal',bins=30)\n",
    "\n",
    "ax[0,1].set_title('Histogram')\n",
    "ax[0,0].set_title('Trace Plot')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for irow in range(ndim):\n",
    "    print(f\"Standard Error of the Mean for theta_{irow}: {samples[:,irow].std()/np.sqrt(samples.shape[0]):.1e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for between chain variations\n",
    "Note how the initial positions of the different walks are chosen here. What does `(-1)**chain` accomplish?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_chains = 2\n",
    "chains = []\n",
    "\n",
    "for ichain in range(no_of_chains):\n",
    "    sampler.reset()\n",
    "    p0 = np.random.rand(nwalkers, ndim)/10 + (-1)**ichain\n",
    "    # Warm-up\n",
    "    if nwarmup > 0:\n",
    "        print(f'Chain {ichain} performing {nwarmup} warmup iterations.')\n",
    "        pos, prob, state = sampler.run_mcmc(p0, nwarmup)\n",
    "        sampler.reset()\n",
    "    else:\n",
    "        pos = p0\n",
    "    \n",
    "    # Perform iterations, starting at the final position from the warmup.\n",
    "    print(f'MH sampler {ichain} performing {nsteps} samples.')\n",
    "    sampler.run_mcmc(pos, nsteps)\n",
    "    print(\"done\")\n",
    "    print(f\"Mean acceptance fraction: {np.mean(sampler.acceptance_fraction):.3f}\")\n",
    "\n",
    "    chains.append(sampler.flatchain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain1 = chains[0]\n",
    "chain2 = chains[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix, ax = plt.subplots(2,1,figsize=(12,10))\n",
    "for icol in range(ndim):\n",
    "    ax[icol].plot(np.arange(chain1.shape[0]),chain1[:,icol])\n",
    "    ax[icol].plot(np.arange(chain2.shape[0]),chain2[:,icol],alpha=.7)\n",
    "    ax[icol].set_ylabel(r'$\\theta_{0}$'.format(icol))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** *What do you conclude from the trace plots?* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot it! (Note how the `posterior` function is used here.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.gca()\n",
    "\n",
    "# Make data.\n",
    "X = np.arange(-4, 4, 0.05)\n",
    "Y = np.arange(-4, 4, 0.05)\n",
    "X, Y = np.meshgrid(X, Y)\n",
    "Z = posterior(Y,X)\n",
    "\n",
    "ax.set_xlim([-1.6,1.6])\n",
    "ax.set_ylim([-1.6,1.6])\n",
    "ax.contour(X, Y, Z, 10)\n",
    "CS = ax.contourf(X, Y, Z, cmap=plt.cm.cubehelix_r);\n",
    "cbar = plt.colorbar(CS)\n",
    "ax.set_aspect('equal', 'box')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PT Sampler\n",
    "\n",
    "Now we repeat the sampling but with a parallel tempering MCMC (or PTMCMC) sampler. We use ptemcee.\n",
    "\n",
    "Here we construct the sampler object that will drive the PTMCMC.\n",
    "Originally the temperature ladder was 21 temperatures separated by factors of 2; this is commented out below. To improve the accuracy of the integration for an evidence calculation, more lower temperatures were added, i.e., a finer grid near $\\beta = 1$.\n",
    "\n",
    "The highest temperature will be $T=1024$, resulting in an effective \n",
    "$\\sigma_T=32\\sigma=3.2$, which is about the separation of our modes. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#ntemps = 21\n",
    "#temps = np.array([np.sqrt(2)**i for i in range(ntemps)])\n",
    "\n",
    "# Modified temperature ladder to improve the integration for evidence calculation\n",
    "# need more low temperatures, i.e. finer grid near beta=1\n",
    "ntemps_lo = 8\n",
    "ntemps_hi = 21\n",
    "temps_lo = np.array([2**(i/8.) for i in range(ntemps_lo)])\n",
    "temps_hi = np.array([np.sqrt(2)**i for i in range(ntemps_hi)])\n",
    "temps = np.concatenate((temps_lo,temps_hi[temps_hi>max(temps_lo)]))\n",
    "ntemps=len(temps)\n",
    "\n",
    "# Let us use 10 walkers in the ensemble at each temperature:\n",
    "nwalkers = 10\n",
    "ndim = 2\n",
    "\n",
    "nburnin = 1000\n",
    "niterations=1000\n",
    "nthin = 10 # only record every nthin iteration\n",
    "\n",
    "nthreads = 1  # threading didn't work with current set up (revisit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betas=1/temps   # define the beta grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the ptemsee sampler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sampler = ptemcee.Sampler(nwalkers, ndim, log_likelihood, log_prior, ntemps,\n",
    "                         threads=nthreads, betas=betas)\n",
    "\n",
    "#Making the sampling multi-threaded is as simple as adding the threads=Nthreads \n",
    "# argument to Sampler. \n",
    "\n",
    "#First, we run the sampler for 1000 burn-in iterations:\n",
    "\n",
    "# initial walkers are normally distributed with mean mup and standard deviation sigp.\n",
    "p0 = np.random.normal(loc=mup, scale=sigp, size=(ntemps, nwalkers, ndim))\n",
    "\n",
    "print(\"Running burn-in phase\")\n",
    "for p, lnprob, lnlike in sampler.sample(p0, iterations=nburnin):\n",
    "    pass\n",
    "sampler.reset()\n",
    "\n",
    "print(\"Running MCMC chains\")\n",
    "#Now we sample for nwalkers*niterations, recording every nthin-th sample:\n",
    "\n",
    "for p, lnprob, lnlike in sampler.sample(p,iterations=niterations, thin=nthin):\n",
    "    pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The resulting samples (nwalkers*niterations/nthin of them)\n",
    "# are stored as the sampler.chain property:\n",
    "\n",
    "assert sampler.chain.shape == (ntemps, nwalkers, niterations/nthin, ndim)\n",
    "\n",
    "# Chain has shape (ntemps, nwalkers, nsteps, ndim)\n",
    "# Zero temperature mean:\n",
    "mu0 = np.mean(np.mean(sampler.chain[0,...], axis=0), axis=0)\n",
    "print(f\"The zero temperature mean is: {mu0}\")\n",
    "\n",
    "(mu1,mu2,sigma1inv,sigma2inv,norm1,norm2)=params_modes\n",
    "print(\"To be compared with likelihood distribution: \")\n",
    "print(f\"... peak 1: {mu1}, peak 2: {mu2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the zero temperature corner plot\n",
    "mcmc_data0 = sampler.chain[0,...].reshape(-1,ndim)\n",
    "figure = corner.corner(mcmc_data0)\n",
    "\n",
    "# Extract the axes\n",
    "axes = np.array(figure.axes).reshape((ndim, ndim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the posterior for four different temperatures\n",
    "ntempered = np.array([ntemps-1,ntemps-11,8,0])\n",
    "\n",
    "for nt in ntempered:\n",
    "    mcmc_data_nt = sampler.chain[nt,...].reshape(-1,ndim)\n",
    "    figure = corner.corner(mcmc_data_nt)\n",
    "    # Extract the axes\n",
    "    axes = np.array(figure.axes).reshape((ndim, ndim))\n",
    "    axes[ndim-1,ndim-1].set_title('T=%i' %temps[nt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
